{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777bcfd1-7339-4772-a813-8fe622efc28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# access data from snowflake\n",
    "import pandas as pd\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": \"\", \n",
    "    \"user\": \"\", \n",
    "    \"password\": \"\",\n",
    "    \"role\": \"ACCOUNTADMIN\",\n",
    "    \"warehouse\": \"HOL_WH\",\n",
    "    \"database\": \"HOL_DB\",\n",
    "    \"schema\": \"PUBLIC\"\n",
    "    }\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "maintenance_df = session.table('maintenance')\n",
    "humidity_df = session.table('humidity')\n",
    "hum_udi_df = session.table('city_udf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c89ceccd-72d0-4a31-9864-5b4ddb54e567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join together the dataframes and prepare training dataset\n",
    "maintenance_city = maintenance_df.join(hum_udi_df, [\"UDI\"])\n",
    "maintenance_hum = maintenance_city.join(humidity_df, (maintenance_city.col(\"CITY\") == humidity_df.col(\"CITY_NAME\"))).select(col(\"TYPE\"), \n",
    "col(\"AIR_TEMPERATURE_K\"), col(\"PROCESS_TEMPERATURE\"), col(\"ROTATIONAL_SPEED_RPM\"), col(\"TORQUE_NM\"), col(\"TOOL_WEAR_MIN\"), col(\"HUMIDITY_RELATIVE_AVG\"), col(\"MACHINE_FAILURE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ac09d5-bea9-4e0e-8dc2-56296137bd3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write training set to snowflake and materialize the data frame into a pandas data frame\n",
    "maintenance_hum.write.mode(\"overwrite\").save_as_table(\"MAINTENANCE_HUM\")\n",
    "maintenance_hum_df = session.table('MAINTENANCE_HUM').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b61ced3-9e9c-4f52-b5c1-6fed5ad0cecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop redundant column\n",
    "maintenance_hum_df = maintenance_hum_df.drop(columns=[\"TYPE\"])\n",
    "maintenance_hum_df.to_csv('maintenance_hum_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bucket if you do not already have one\n",
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "bucket = client.bucket('<UNIQUE BUCKET NAME>')\n",
    "bucket.location = '<GCP REGION>'\n",
    "bucket.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1b2f02-1d36-4971-a943-756e59510432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create functions to train model and register to vertexai\n",
    "from google.cloud import aiplatform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "def train_and_save_model(dataframe, target_column, model_filename):\n",
    "    # Split the data into features and target\n",
    "    X = dataframe.drop(target_column, axis=1)\n",
    "    y = dataframe[target_column]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # Save the model to a file locally\n",
    "    joblib.dump(model, model_filename)\n",
    "    return model_filename\n",
    "\n",
    "def upload_model_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    # Add a verification check\n",
    "    if blob.exists():\n",
    "        print(f\"Verification: The file {destination_blob_name} exists in the bucket.\")\n",
    "    else:\n",
    "        print(f\"Verification failed: The file {destination_blob_name} does not exist in the bucket.\")\n",
    "\n",
    "def register_model(project_id, location, model_display_name, model_directory_uri):\n",
    "    # Initialize the Vertex AI client\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "\n",
    "    # Wait for a short period to ensure the file is visible in GCS\n",
    "    print(\"Waiting for the model files to become consistent in GCS...\")\n",
    "    time.sleep(30)  # Wait for 30 seconds\n",
    "\n",
    "    # Register the model\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=model_directory_uri,  # This should point to the directory\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    "    )\n",
    "\n",
    "    print(f\"Model {model.display_name} has been registered in Vertex AI.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f11782-7c3b-4e61-90a1-743e2732b741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# usage\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('maintenance_hum_df.csv')  # Modify with your data source\n",
    "    target_column = 'MACHINE_FAILURE'  # Modify with your target column\n",
    "    model_filename = 'model.joblib'\n",
    "\n",
    "    # Train the model and save it locally\n",
    "    saved_model_filename = train_and_save_model(df, target_column, model_filename)\n",
    "\n",
    "    # Specify your GCP project details\n",
    "    project_id = '<GCP PROJECT ID>'  # Modify with your GCP project ID\n",
    "    location = '<GCP DEPLOYMENT REGION>'  # Modify with your region, e.g., 'us-central1'\n",
    "    bucket_name = '<BUCKET NAME>'  # Your Google Cloud Storage bucket name\n",
    "    model_display_name = 'failure_update'  # Modify with your desired model name\n",
    "\n",
    "    # Define the Google Cloud Storage path for the model DIRECTORY\n",
    "    model_directory_uri = f\"gs://{bucket_name}/\"  # Ensure this ends with a slash\n",
    "\n",
    "    # Upload the model to Google Cloud Storage\n",
    "    upload_model_to_bucket(bucket_name, saved_model_filename, model_filename)\n",
    "\n",
    "    # Register the model in Vertex AI\n",
    "    registered_model = register_model(project_id, location, model_display_name, model_directory_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ff12d01-aed9-48cd-86fc-c7f1c408f004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_model_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name.strip('/'))  # Ensure no leading/trailing slash\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"Downloaded storage object {bucket_name}/{source_blob_name} to local file {destination_file_name}.\")\n",
    "\n",
    "def load_model_from_vertex_ai(project_id, location, model_id):\n",
    "    # Initialize the Vertex AI client\n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "\n",
    "    # Retrieve the model details\n",
    "    model = aiplatform.Model(model_name=f\"projects/{project_id}/locations/{location}/models/{model_id}\")\n",
    "    model_artifacts_gcs_uri = model.gca_resource.artifact_uri\n",
    "    print(f\"Model artifacts located at: {model_artifacts_gcs_uri}\")\n",
    "\n",
    "    # Parse the GCS URI to get the bucket and path\n",
    "    bucket_name = model_artifacts_gcs_uri.split(\"/\")[2]\n",
    "    model_path_in_bucket = \"/\".join(model_artifacts_gcs_uri.split(\"/\")[3:]).strip('/')  # Avoid leading slash\n",
    "\n",
    "    # Download the model artifact (.joblib file)\n",
    "    destination_file_name = \"downloaded_model.joblib\"  # Local file to which the model will be downloaded\n",
    "    download_model_from_gcs(bucket_name, model_path_in_bucket + 'model.joblib', destination_file_name)\n",
    "\n",
    "    # Load the model into memory\n",
    "    loaded_model = joblib.load(destination_file_name)\n",
    "    print(\"Model loaded into memory successfully.\")\n",
    "\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723381b-c78c-4302-a946-54afa96bfedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    project_id = '<GCP PROJECT ID>'  # Replace with your Google Cloud project ID\n",
    "    location = '<GCP REGION>'  # Replace with your model's region, e.g., 'us-central1'\n",
    "    model_id = '<REGISTERED MODEL ID>'  # Replace with your model ID\n",
    "\n",
    "    model = load_model_from_vertex_ai(project_id, location, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ccb5f-6e62-45a2-818d-ea2b0d877fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e4dde-811c-4d78-839e-9e51d7d9184f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# connect to Snowpark registry and log model\n",
    "from snowflake.ml.registry import registry\n",
    "test_data = maintenance_hum_df.drop('MACHINE_FAILURE', axis=1)\n",
    "reg = registry.Registry(session=session)\n",
    "reg.log_model(model, model_name='vertex_model', version_name='v1', sample_input_data=test_data)\n",
    "\n",
    "# verify model deployment and view functions associated with the model\n",
    "mv = reg.get_model('vertex_model').version('v1')\n",
    "mv.show_functions()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-python38-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-env-python38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
